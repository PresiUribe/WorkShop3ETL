# ğŸŒ Happiness Score Prediction with Kafka + Machine Learning

ETL Workshop 3 - Data Engineering and Artificial Intelligence  
Universidad AutÃ³noma de Occidente

## ğŸ“‹ Project Overview

This project implements an end-to-end ML pipeline that combines **Data Streaming (Apache Kafka)** with **Machine Learning** to predict happiness scores across different countries and years. The system trains a regression model on World Happiness Report data, streams transformed data through Kafka, and stores predictions in a MySQL database for analysis.

## ğŸ—ï¸ System Architecture

```
CSV Files â†’ EDA â†’ Model Training â†’ .pkl Model
                                      â†“
Test Data â†’ Kafka Producer â†’ Kafka Topic â†’ Kafka Consumer â†’ Predictions
                                                  â†“
                                            Load Model (.pkl)
                                                  â†“
                                            MySQL Database
```

## ğŸ¯ Key Features

- **ETL Pipeline**: Extract, Transform, Load data from multiple CSV sources
- **Machine Learning**: Train regression model (70/30 split) to predict happiness scores
- **Real-time Streaming**: Stream data using Apache Kafka
- **Prediction System**: Load trained model and predict on streaming data
- **Database Storage**: Store features, actual scores, and predictions in MySQL
- **Performance Metrics**: RÂ², MAE, RMSE evaluation

## ğŸ“ Project Structure

```
happiness-kafka-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Place your 5 CSV files here
â”‚   â””â”€â”€ processed/              # Processed data (auto-generated)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ happiness_model.pkl     # Trained model (auto-generated)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Configuration file
â”‚   â”œâ”€â”€ train_model.py         # Model training script
â”‚   â”œâ”€â”€ kafka_producer.py      # Kafka producer
â”‚   â””â”€â”€ kafka_consumer.py      # Kafka consumer
â”œâ”€â”€ database/
â”‚   â””â”€â”€ create_database.sql    # SQL script to create database
â”œâ”€â”€ notebooks/                  # Jupyter notebooks (optional)
â”œâ”€â”€ visualizations/            # Charts and visualizations
â”œâ”€â”€ .env                       # Environment variables (create this)
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Setup Instructions

### 1. Prerequisites

- Python 3.7+
- MySQL (with Workbench)
- Apache Kafka (installed and running)
- WSL (if using Windows)

### 2. Clone/Download the Project

```bash
git clone <your-repo-url>
cd happiness-kafka-project
```

### 3. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Environment Variables

Create a `.env` file in the project root:

```bash
cp .env.example .env
```

Edit `.env` with your MySQL credentials:

```env
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_password_here
DB_NAME=happiness_predictions

KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=happiness-data

MODEL_PATH=models/happiness_model.pkl
```

### 5. Setup MySQL Database

Run the SQL script in MySQL Workbench:

```bash
mysql -u root -p < database/create_database.sql
```

Or open `database/create_database.sql` in MySQL Workbench and execute it.

### 6. Add Your Data

Place your 5 CSV files in the `data/raw/` directory:
- 2015.csv
- 2016.csv
- 2017.csv
- 2018.csv
- 2019.csv (or whatever years you have)

## ğŸ“Š Running the Project

### Step 1: Start Kafka Services

Open **3 separate CMD/Terminal windows**:

**Terminal 1 - Start Zookeeper:**
```bash
cd C:\kafka  # or your Kafka installation path
.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties
```

**Terminal 2 - Start Kafka Server:**
```bash
cd C:\kafka
.\bin\windows\kafka-server-start.bat .\config\server.properties
```

**Terminal 3 - Create Kafka Topic:**
```bash
cd C:\kafka
.\bin\windows\kafka-topics.bat --create --topic happiness-data --bootstrap-server localhost:9092
```

### Step 2: Train the Model

```bash
cd src
python train_model.py
```

This will:
- Load and combine all CSV files
- Preprocess the data
- Train the model (70/30 split)
- Save the model as `models/happiness_model.pkl`
- Save test data to `data/processed/test_data.csv`
- Print evaluation metrics (RÂ², MAE, RMSE)

### Step 3: Start Kafka Consumer

Open a new terminal:

```bash
cd src
python kafka_consumer.py
```

The consumer will:
- Load the trained model
- Connect to MySQL database
- Wait for incoming messages from Kafka

### Step 4: Start Kafka Producer

Open another terminal:

```bash
cd src
python kafka_producer.py
```

The producer will:
- Load test data
- Stream records to Kafka one by one
- Display progress

### Step 5: Monitor Results

Watch the consumer terminal for real-time predictions:

```
[Message 1] Denmark (2016)
  Actual Score:    7.5260
  Predicted Score: 7.5143
  Error:           0.0117
  âœ“ Saved to database
```

## ğŸ“ˆ Evaluation Metrics

The model is evaluated using:
- **RÂ² Score**: Coefficient of determination
- **MAE (Mean Absolute Error)**: Average absolute difference
- **RMSE (Root Mean Squared Error)**: Square root of average squared differences

## ğŸ—„ï¸ Database Schema

**Table: `predictions`**

| Column | Type | Description |
|--------|------|-------------|
| id | INT | Primary key (auto-increment) |
| country | VARCHAR(255) | Country name |
| region | VARCHAR(255) | Geographic region |
| year | VARCHAR(10) | Year of data |
| economy_gdp | FLOAT | GDP per capita |
| family | FLOAT | Family support score |
| health_life_expectancy | FLOAT | Life expectancy score |
| freedom | FLOAT | Freedom score |
| trust_government | FLOAT | Government trust score |
| generosity | FLOAT | Generosity score |
| dystopia_residual | FLOAT | Dystopia residual score |
| actual_happiness_score | FLOAT | Actual happiness score |
| predicted_happiness_score | FLOAT | ML predicted score |
| prediction_error | FLOAT | Absolute error |
| timestamp | TIMESTAMP | Record creation time |

## ğŸ“Š Query Examples

```sql
-- View all predictions
SELECT * FROM predictions ORDER BY timestamp DESC LIMIT 10;

-- Average prediction error by country
SELECT country, AVG(prediction_error) as avg_error
FROM predictions
GROUP BY country
ORDER BY avg_error;

-- Compare actual vs predicted scores
SELECT country, year, 
       actual_happiness_score, 
       predicted_happiness_score,
       prediction_error
FROM predictions
WHERE year = '2016'
ORDER BY actual_happiness_score DESC;
```

## ğŸ› ï¸ Technologies Used

- **Python 3.x**: Main programming language
- **Pandas & NumPy**: Data manipulation
- **Scikit-learn**: Machine learning (LinearRegression)
- **Apache Kafka**: Data streaming
- **kafka-python**: Python Kafka client
- **MySQL**: Database storage
- **mysql-connector-python**: MySQL Python driver
- **python-dotenv**: Environment variables management
- **Joblib**: Model serialization

## ğŸ“ Learning Objectives Achieved

âœ… Conduct EDA on multiple datasets  
âœ… Perform ETL processes  
âœ… Engineer features for regression modeling  
âœ… Train and evaluate ML model (70/30 split)  
âœ… Implement Kafka-based streaming system  
âœ… Use serialized model for predictions  
âœ… Store results in database  
âœ… Compute performance metrics  

## ğŸ› Troubleshooting

### Kafka Connection Issues
- Ensure Zookeeper and Kafka are running
- Check ports 2181 (Zookeeper) and 9092 (Kafka)
- Verify `KAFKA_BOOTSTRAP_SERVERS` in `.env`

### Database Connection Issues
- Verify MySQL is running
- Check credentials in `.env` file
- Ensure database `happiness_predictions` exists

### Model Not Found
- Run `train_model.py` first
- Check `MODEL_PATH` in `.env`

### Import Errors
- Ensure all dependencies are installed: `pip install -r requirements.txt`
- Activate virtual environment if using one

## ğŸ“ Next Steps

1. **Visualizations**: Create dashboards in PowerBI/Tableau/Looker
2. **Advanced Models**: Try RandomForestRegressor or XGBoost
3. **Feature Engineering**: Add more derived features
4. **Hyperparameter Tuning**: Optimize model parameters
5. **Real-time Monitoring**: Add logging and monitoring

## ğŸ‘¥ Authors

- Your Name
- Universidad AutÃ³noma de Occidente
- Data Engineering and AI Program

## ğŸ“„ License

This project is part of the ETL course (Workshop 3).

---

**Happy Coding! ğŸš€**
